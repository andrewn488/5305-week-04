---
title: "week_04_notes"
author: "Andrew Nalundasan"
date: "10/21/2021"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# load libraries
library(readxl)
library(forecast)
```


## Overview video

+ Recursive, Rolling, and Fixed environments

    + Keep this information in the back of my mind
    + Won't use these schemes until week 9 of the course
    

## Chapter 6 Lecture Videos

### Moving Average Models

+ White Noise - error term
    
    + mean != 0
    + doesn't need to have a normal distribution
    + just needs to be stationary with AC=0 and PC=0 
    + Autocorrelation == 0
    + Partial Correlation == 0
    
+ Trend or Seasonality

    + Wolt Theorem
        + for any stationary process, we can use a linear model to process it to be stationary
        + this is our starting point
    + stationary TS
  
+ White Noise in the real world

    + Constant mean, somewhere ~ 0 and frequently returns to its mean
    + no spikes -- all close to 0
    + difficult to predict individual stock price because of white noise
        + usually returns to 0 at some point

+ Moving average process

    + Constant mean and stationary, very similar to white noise
    + MA is a little more sticky
        + when it's up, it's stays up. when it's down it stays down
        + but then returns back to 0 at some point
    + MA is the model in TS: only have 1 regressor
        + error term of past periods
    + as theta increases, MA becomes more "sticky" 
    + as theta incrases, MA becomes more volatile and variance increases
    + counting the number of spikes tell us what order it's in
    
+ MA1 process estimate and forecast (5-year Treasury Note Yield)

    + White Noise or Moving Average?
        + Graph the TS
        + Check ACF and PACF
            + How many spikes?
        + If 1 spike, use MA1 model
    + MA 1 model is very "short memory"
    
+ Higher orders of MA process

    + Have 2+ coefficients
    + Becomes more "sticky" 
    + MA2 process will have 2 spikes in ACF
        + PACF will be more gradual as long as the coefficient is positive
        + When coefficient is positive, PACF oscilates
        + When coefficient is negative, PACF alternates 

+ Summary

    + started the MA concepts
    + similarity between MA concept and making estimates and forecasts
    + MA model is **always** stationary process 

## Chapter 6 R Instructions

### Part I: simulate MA time series

+ MA1 vs. MA2

```{r}
# simulate y1
# MA1
# generate 200 observations
# because MA1, only need 1 regressor
# innovation has normal distribution with mean=0, stdev=0.5, unconditional mean=2
y1 <- arima.sim(n=200, list(ma = 0.5), innov = 0.5*rnorm(200)) + 2

# plot time-series
plot.ts(y1)

```

```{r}

# simulate y2
# MA2 so need to provide 2 coefficients
# first regressor: epsilon_t-1=-1, second regressor: epsilon_t-2=0.25
y2 <- arima.sim(n=200, list(ma = c(-1, 0.25)), innov = 0.5 * rnorm(200)) + 2

# plot time-series
plot.ts(y2)
```





```{r}
# this doesn't work well in markdown view.
# this renders differently in console while running from top to bottom
# par(mar = c(1, 1, 1, 1))

layout(matrix(c(1, 1, 2, 2), 2, 2, byrow=TRUE))
layout(matrix(c(1, 1, 1, 2), 2, 2, byrow=TRUE))

plot.ts(y1)
plot.ts(y2)

# this command clears out the console pane 
# dev.off()
```

**Comments**

+ MA2 seems to return to 0 more often than MA1
+ the first figure occupies the first row
+ the second figure occupies the second row


### Part II: estimation 

```{r}
# load data: 5 year-treasure yield

data <- read_excel("../02_raw_data/Figure6_5_Table6_1_treasury.xls")

dy <- ts(data$DY, frequency = 12, start = c(1953, 4))

plot(dy)
```

**Comments**

+ could be MA1 or MA2, we don't know


```{r}
# use 'forecast' package

# get subset of the data and make it the estimation data
dysub <- data$DY[1:656]

# make into TS
dysub <- ts(dysub, frequency = 12, start = c(1953, 4))

# get summary  
summary(dysub)

# make model
# order:
    # 0 = AR model
    # 0 = Order of difference
    # 1 = order of the moving average
model <- arima(dysub, order=c(0, 0, 1))

# get summary
summary(model)
```


### Part III:forecasting


```{r}
# make forecasts
# h is the forecasting horizon
    # from 1 step ahead, 2 steps ahead..., 5 steps ahead
    # this returns 5 forecasts for us
fcasts <- forecast(model, h = 5)

# get summary
summary(fcasts)

# plot forecast and color lines red
plot(fcasts, include = 10)
lines(dy, col="red")
```

**Comments**

+ dark bands - interval bands
+ dark grey - 95% confidence interval
+ light grey - 80% of confidence interval
+ compare blue line (forecast) vs. red line (original) 

### Summary

+ Using R to simulate MA TS models
+ How to estimate and forecast TS using MA model

## Week 4 External Videos

Notes in OneNote

## Week 4 Videos on my notes

1. White Noise process

+ if every random variable in the process has mean == 0
+ and covariance == 0 between any random variable
+ --> then it's white noise
+ for such a process to be stationary:

    1. E(Xt) == 0
    2. var(Xt) = E(Xt**2) = sigma**2 < infinity
    3. R(tau) = cov(Xt, Xs) = {sigma**2: tau=0, 0: tau!=0}
    
2. Wold Decomposition Theorem

+ gives us the justification for autoregressive moving average model (ARMA) 

    + Any stationary process {Xt} can be expressed as Xt = dt + vt
        + dt <- perfectly predictable process
            1.dt can be perfectly predicted by its own past
                + dt = E[dt | dt-1, dt-2, dt-3, ...]
        + vt <- white noise
            1. vt has the one-side linear representation
                + vt = sum(white noise processes)
                
+ An ARMA is a discrete process {Xt} such that Ap(L) Xt = Gq(L) Et with Et ~ iid (0, sigma**2)

    + This is a demeaned version of ARMA process

                
3. MA Process aka ARMA(0, q=1)

+ ARMA(p, q)

    + (1 - A1L - A2L**2...ApL**p) Xt = (1 + b1L + b2L**2 ... + bqL**q) Epsilon_t
    
+ A moving process of order q >= 0, MA(q) has the following formula:

    + Yt = mu + b1 * Epsilon_t-1 + b2 * Epsilon_t-2 +...+ bq * Epsilon_t-q


4. MA(1) Process

+ demeaned version

+ Properties: 

    + stationary process
    1. Mean == 0
    2. Variance (sigma**2) == 
    3. Covariance == 0
    4. Correlation Coefficient (rho) == 

5. Invertibility

+ A process {Xt} (MA process) is invertible if it can be expressed as an infinite order of AutoRegressive process
+ for MA(1) process Xt = Epsilon_t + theta * Epsilon_t-1 to be invertible abs(theta) < 1
    
    + if theta is < 1, then it's invertible


6. Optimal Forecasting of MA(1) Process 

(1) 1-step ahead optimal forecast
(2) 1-step ahead optimal forecast error
(3) Calculate variance of error
(4) Density forecast


7. MA(q) Process

Read PDF notes


8. MA(infinity)

Read PDF notes

9. MA(q) Forecasting

Read PDF notes
